{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport google.generativeai as genai\nfrom google.generativeai.types import HarmCategory, HarmBlockThreshold\nimport os\nimport time\nimport re\nimport random\nfrom tqdm import tqdm\nfrom kaggle_secrets import UserSecretsClient # Import for Kaggle Secrets\n\n# --- 1. Gemini API Configuration ---\n\n# Inserisci le tue API Key qui come una lista.\n# Assicurati che siano tutte chiavi valide e provenienti da progetti/account diversi per avere quote separate.\n# Puoi anche caricarle da Kaggle Secrets, una per una, o concatenare più segreti.\ntry:\n    user_secrets = UserSecretsClient()\n    \n    # Esempio: recupera più chiavi se le hai salvate con nomi diversi in Kaggle Secrets\n    # Oppure recupera una singola chiave per testare e poi aggiungine altre\n    API_KEYS = [\n        user_secrets.get_secret(\"GOOGLE_API_KEY_1\"),\n        user_secrets.get_secret(\"GOOGLE_API_KEY_2\"),\n        user_secrets.get_secret(\"GOOGLE_API_KEY_3\"),\n        user_secrets.get_secret(\"GOOGLE_API_KEY_4\"),\n        user_secrets.get_secret(\"GOOGLE_API_KEY_5\"),\n        # Aggiungi qui tutte le tue chiavi\n    ]\n    \n    # Rimuovi eventuali chiavi None nel caso in cui un segreto non sia stato trovato\n    API_KEYS = [key for key in API_KEYS if key]\n\n    if not API_KEYS:\n        raise ValueError(\"Nessuna GOOGLE_API_KEY valida trovata in Kaggle Secrets. Assicurati di averne almeno una.\")\n    \n    print(f\"Trovate {len(API_KEYS)} API Key di Gemini da Kaggle Secrets.\")\n    \n    # Inizializza con la prima chiave\n    current_api_key_index = 0\n    genai.configure(api_key=API_KEYS[current_api_key_index])\n    print(f\"Configurazione API Key iniziale (chiave {current_api_key_index + 1}).\")\n\nexcept Exception as e:\n    print(f\"Errore nella configurazione delle API Key di Gemini: {e}. Assicurati che 'GOOGLE_API_KEY_1' (e altre) siano impostate in Kaggle Secrets.\")\n    exit()\n\n# --- 2. Generation Parameters and Prompt Definition ---\n\n# Parametri per massimizzare la creatività\ngeneration_config = {\n    \"temperature\": 1.2, # Adjusted for balanced creativity and coherence\n    \"top_p\": 1.0,       # Broader token selection\n    \"top_k\": 0,         # No top-k filtering applied\n    \"response_mime_type\": \"text/plain\" # Ensures plain text output\n}\n\n# Impostazioni di sicurezza rigorose per bloccare contenuti inappropriati\nsafety_settings = {\n    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n}\n\n# Inizializza il modello generativo con le impostazioni di sicurezza (viene riconfigurato in caso di cambio chiave)\nmodel = genai.GenerativeModel(\n    model_name='gemini-2.0-flash-lite',\n    generation_config=generation_config,\n    safety_settings=safety_settings\n)\nprint(\"Modello AI generativo 'gemini-2.0-flash-lite' inizializzato.\")\n\n\n# Categorie di ruoli sani per guidare la generazione - IN ITALIANO\nHEALTHY_CONVERSATION_TYPES = {\n    'Supporto Reciproco': ('Supportivo', 'Incoraggiante'),\n    'Risoluzione Costruttiva dei Problemi': ('Proattivo', 'Collaborativo'),\n    'Vulnerabilità Emotiva e Accettazione': ('Vulnerabile', 'Empatico'),\n    'Risoluzione dei Conflitti': ('Riflessivo', 'Comprensivo'),\n    'Apprezzamento e Gratitudine': ('Grato', 'Riconoscente'),\n    'Condivisione di Hobby/Interessi': ('Entusiasta', 'Curioso'),\n    'Pianificazione Eventi Futuri': ('Organizzato', 'Flessibile'),\n    'Battute Leggere e Scherzose': ('Giocoso', 'Reattivo')\n}\n\n\ndef generate_conversation_prompt(conversation_type_name, role_one, role_two):\n    \"\"\"\n    Crea un prompt dettagliato per l'LLM per generare una conversazione sana.\n    \"\"\"\n    return f\"\"\"\n**Compito:**\nGenera una conversazione realistica e sana tra due partner di una coppia, basata sui ruoli assegnati e sul tipo di conversazione.\n\n**Tipo di Conversazione:**\n{conversation_type_name}\n\n**Istruzioni:**\n1.  **Inventa due nomi distinti italiani, nuovi e diversi per ogni conversazione,** per i partner (un uomo e una donna).\n2.  **Assegna i ruoli:** un partner incarnerà il ruolo \"{role_one}\", l'altro \"{role_two}\".\n3.  **Crea un dialogo** (6-10 battute, ovvero 3-5 turni per ogni parlante) che rifletta autenticamente un'interazione costruttiva e non tossica, allineata con la dinamica '{conversation_type_name}'.\n    * Assicurati che la conversazione sia completamente priva di qualsiasi linguaggio dannoso, offensivo, volgare, di odio, minaccioso, di cyberbullismo o tentativi di controllo.\n    * Il dialogo deve essere chiaro e inequivocabile nel suo intento non tossico.\n\n**Formato di Output Richiesto (due parti distinte separate da '|||'):**\n\nNOMI: [Nome 1], [Nome 2]\n|||\nDIALOGO:\n[Inizia il dialogo qui. Ogni battuta su una nuova riga, iniziando con il nome del parlante seguito dai due punti.\nEsempio:\nMarco: Ciao! Come stai?\nGiulia: Ciao! Tutto bene, grazie.]\n\"\"\"\n\n\ndef parse_llm_response(response_text):\n    \"\"\"\n    Estrae nomi e dialogo dalla risposta dell'LLM basandosi sul formato definito.\n    Gestisce elegantemente potenziali errori di parsing.\n    \"\"\"\n    try:\n        parts = response_text.split('|||')\n        if len(parts) != 2:\n            raise ValueError(f\"Formato di risposta non valido. Attese 2 parti separate da '|||', ottenute {len(parts)}.\")\n\n        # Estrai nomi\n        names_part = parts[0].replace('NOMI:', '').strip()\n        name1, name2 = [name.strip() for name in names_part.split(',')]\n\n        # Estrai e formatta il dialogo\n        dialogue_part = parts[1].replace('DIALOGO:', '').strip()\n        dialogue_lines = [line.strip() for line in dialogue_part.split('\\n') if line.strip()]\n\n        # Ricomponi le righe del dialogo nel formato desiderato\n        formatted_dialogue = []\n        for line in dialogue_lines:\n            if ':' in line:\n                speaker_name, message = line.split(':', 1) # Dividi solo al primo due punti\n                formatted_dialogue.append(f'\"{speaker_name.strip()}: {message.strip()}\"')\n            else:\n                # Se una riga non ha i due punti, includila semplicemente come messaggio quotato se non è vuota\n                if line.strip():\n                    formatted_dialogue.append(f'\"{line.strip()}\"')\n\n        final_dialogue_string = ' '.join(formatted_dialogue)\n\n        if not final_dialogue_string:\n            raise ValueError(\"Il dialogo parsato è vuoto o troppo corto.\")\n\n        return {\n            'name1': name1.title(),\n            'name2': name2.title(),\n            'conversation': final_dialogue_string,\n        }\n    except Exception as e:\n        print(f\"[ERRORE DI PARSING] Impossibile parsare la risposta dell'LLM: {e}\\nRisposta Grezza:\\n---\\n{response_text}\\n---\")\n        return None\n\n\n# --- 3. Processo di Generazione del Dataset ---\n\nNUM_SAMPLES_TO_GENERATE = 1000 # Numero target di conversazioni sane\noutput_dataset_filepath = \"healthy_dataset.csv\" # Salvato nella root, nuovo nome\nfinal_df_columns = ['conversation_type', 'name1', 'name2', 'conversation']\n\nprint(f\"\\n--- Avvio Generazione di {NUM_SAMPLES_TO_GENERATE} Conversazioni Sane ---\")\n\nprogress_bar = tqdm(range(NUM_SAMPLES_TO_GENERATE), desc=\"Generando dialoghi sani\")\nsuccessful_generations_count = 0\n\nfor i in progress_bar:\n    # Seleziona casualmente un tipo di conversazione e i suoi ruoli associati\n    conversation_type, (role1, role2) = random.choice(list(HEALTHY_CONVERSATION_TYPES.items()))\n\n    # Crea il prompt per la generazione corrente\n    current_prompt = generate_conversation_prompt(conversation_type, role1, role2)\n\n    retries_current_key = 0\n    max_retries_per_key = 3 # Tentativi prima di cambiare chiave\n    \n    while True: # Ciclo per riprovare con la stessa chiave o cambiarla\n        try:\n            # Chiama l'API di Gemini\n            response_obj = model.generate_content(current_prompt)\n            \n            # Controlla se la risposta API contiene parti valide (non bloccate dai filtri di sicurezza)\n            if response_obj.parts:\n                parsed_data = parse_llm_response(response_obj.text)\n                \n                if parsed_data:\n                    parsed_data['conversation_type'] = conversation_type # Aggiungi il tipo di conversazione\n                    \n                    # Crea un DataFrame a riga singola\n                    current_row_df = pd.DataFrame([parsed_data])\n                    current_row_df = current_row_df[final_df_columns] # Forza l'ordine delle colonne\n\n                    # Determina se l'intestazione deve essere scritta (solo se il file non esiste o è vuoto)\n                    write_header = not os.path.exists(output_dataset_filepath) or \\\n                                   (os.path.exists(output_dataset_filepath) and os.path.getsize(output_dataset_filepath) == 0)\n                    \n                    # Aggiungi il dialogo generato al file CSV\n                    current_row_df.to_csv(output_dataset_filepath, mode='a', header=write_header, index=False, encoding='utf-8')\n                    \n                    successful_generations_count += 1\n                    progress_bar.set_postfix_str(f\"Salvato dialogo #{successful_generations_count} (Tipo: {conversation_type}) - Chiave {current_api_key_index + 1}/{len(API_KEYS)}\")\n\n                    # --- Stampa il dialogo generato nell'output ---\n                    print(f\"\\n--- Dialogo Generato #{successful_generations_count} (Tipo: {conversation_type}) ---\")\n                    print(f\"Nomi: {parsed_data['name1']}, {parsed_data['name2']}\")\n                    print(\"Conversazione:\")\n                    for turn in parsed_data['conversation'].split(' \"'):\n                        turn = turn.strip()\n                        if turn:\n                            if turn.startswith('\"') and turn.endswith('\"'):\n                                turn = turn[1:-1]\n                            print(f\"  {turn.strip()}\")\n                    print(\"--------------------------------------------------\")\n                    break # Esci dal ciclo while se la generazione è riuscita\n                else:\n                    progress_bar.set_postfix_str(\"Impossibile parsare la risposta LLM, saltando l'elemento.\")\n                    break # Esci dal ciclo while se il parsing fallisce\n            else:\n                # Gestisci i casi in cui la risposta API è vuota o bloccata\n                if hasattr(response_obj, 'prompt_feedback') and response_obj.prompt_feedback.block_reason:\n                    progress_bar.set_postfix_str(f\"Risposta LLM bloccata per filtri di sicurezza: {response_obj.prompt_feedback.block_reason}. Saltando.\")\n                    print(f\"\\n[AVVISO] Risposta LLM bloccata (Iterazione {i+1}): {response_obj.prompt_feedback.block_reason}\")\n                else:\n                    progress_bar.set_postfix_str(\"Risposta vuota o non valida dall'LLM. Saltando.\")\n                    print(f\"\\n[AVVISO] Risposta vuota o non valida dall'LLM (Iterazione {i+1}).\")\n                break # Esci dal ciclo while se la risposta è bloccata o vuota\n\n        except Exception as e:\n            error_message = str(e)\n            if \"429 You exceeded your current quota\" in error_message:\n                retries_current_key += 1\n                print(f\"\\n[ERRORE] Quota Superata per Chiave {current_api_key_index + 1}. Tentativo {retries_current_key}/{max_retries_per_key}.\")\n                \n                if retries_current_key >= max_retries_per_key:\n                    current_api_key_index += 1 # Passa alla chiave successiva\n                    retries_current_key = 0 # Azzera i tentativi per la nuova chiave\n                    \n                    if current_api_key_index < len(API_KEYS):\n                        genai.configure(api_key=API_KEYS[current_api_key_index])\n                        print(f\"Passato alla Chiave API successiva: {current_api_key_index + 1}/{len(API_KEYS)}.\")\n                    else:\n                        print(f\"\\n[ERRORE FATALE] Tutte le API Key hanno esaurito la quota o sono state bloccate. Interruzione.\")\n                        exit() # Esci dal programma se tutte le chiavi sono esaurite\n                time.sleep(5) # Pausa più lunga prima di riprovare con la stessa chiave o con la nuova\n            else:\n                print(f\"\\n[ERRORE] Si è verificato un errore inatteso durante la chiamata API (Iterazione {i+1}): {error_message}\")\n                progress_bar.set_postfix_str(\"Errore chiamata API, saltando.\")\n                break # Esci dal ciclo while per errori non di quota\n        \n    time.sleep(1.5) # Ritardo per rispettare i limiti di velocità (importante per il free tier)\n\n# --- 4. Riepilogo Finale ---\n\nprint(\"\\n--- Generazione Conversazioni Sane Conclusa ---\")\n\nif successful_generations_count > 0:\n    print(f\"Aggiunti {successful_generations_count} nuovi dialoghi sani a: '{output_dataset_filepath}'\")\n    print(\"Esecuzione completata con successo.\")\nelse:\n    print(\"Nessun dialogo sano è stato generato in questa sessione. Controlla le API Key, le quote o le impostazioni di sicurezza.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T22:49:36.106497Z","iopub.execute_input":"2025-07-09T22:49:36.106744Z","iopub.status.idle":"2025-07-09T23:03:14.349453Z","shell.execute_reply.started":"2025-07-09T22:49:36.106726Z","shell.execute_reply":"2025-07-09T23:03:14.348323Z"}},"outputs":[],"execution_count":null}]}